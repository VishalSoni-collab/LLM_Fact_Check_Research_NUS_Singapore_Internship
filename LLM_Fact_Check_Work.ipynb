{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain, backoff, openai, requests, urllib.parse\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.tools import HumanInputRun\n",
    "from apikeys import WolframAplha_KEY, SerpAPI_KEY\n",
    "from langchain.agents import Tool\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from apikeys import OPENAI_KEY\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(temperature=0)\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = WolframAplha_KEY\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\"\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "wolfram = WolframAlphaAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39660816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_file():\n",
    "    # Create a root Tk window and hide it\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Open a file dialog and get the path to the selected file\n",
    "    file_path = filedialog.askopenfilename()\n",
    "\n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        # Read .txt file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    " \n",
    "    elif file_extension == '.docx':\n",
    "        # Read .docx file\n",
    "        text = docx2txt.process(file_path)\n",
    "\n",
    "    elif file_extension == '.pdf':\n",
    "        # Read .pdf file\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text()\n",
    "\n",
    "    elif file_extension in ['.xls', '.xlsx']:\n",
    "        # Read Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        text = df.to_string()\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file extension: {file_extension}\")\n",
    "        text = \"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def calculate_score(true_count, total_count):\n",
    "    \"\"\"Calculate the verification score.\"\"\"\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    return true_count / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd841bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "\n",
    "def breakdown_answer(answer):\n",
    "    # Split the input paragraph into sentences\n",
    "    sentences = re.split(r'\\.', answer)\n",
    "\n",
    "    # Remove any empty strings resulting from split\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    # Now iterate over each sentence and send to OpenAI API\n",
    "    for sentence in sentences:\n",
    "        prompt = f\"\"\"You are a highly advanced language model programmed to decompose complex text into simpler factoids. Your primary task is to dissect larger narratives into standalone pieces of information, with each piece containing a single fact that can be independently verified. \n",
    "\n",
    "    Your goal is to retain the original meaning and context of the information without making any corrections or alterations. This includes both explicit details and implied facts present in the original text. Additionally, you should be able to identify and illustrate dependencies between factoids, creating a hierarchy that reflects the nested structure of the facts.\n",
    "\n",
    "    Even if the information appears incorrect, your task is not to correct it, but to reproduce it faithfully in a simpler, structured form that can be easily verified or refuted.\n",
    "\n",
    "    Apply this to the following statement:\n",
    "    {sentence}\n",
    "    Output only the sentences - one in each line.\n",
    "        \"\"\"\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        raw_chunks = response.choices[0].text.strip().split(\"\\n\")\n",
    "\n",
    "        # Split raw chunks by full stop and comma\n",
    "        for chunk in raw_chunks:\n",
    "            sub_chunks = re.split(r'[.,;]', chunk)\n",
    "            # Filter out chunks that are purely numerical and add to the list\n",
    "            chunks.extend([sub.strip() for sub in sub_chunks if sub.strip() and not sub.strip().isdigit()])\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60309dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(true_count, total_count):\n",
    "    \"\"\"Calculate the verification score.\"\"\"\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    return true_count / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SummaryIndex, RssReader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.document_loaders import TextLoader\n",
    "from llama_index import SummaryIndex, SimpleWebPageReader  \n",
    "\n",
    "def calculate_score(true_count, total_count):\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    return true_count / total_count\n",
    "\n",
    "def get_relevant_passages_qdrant(qdrant, fact_chunk):\n",
    "    found_docs = qdrant.max_marginal_relevance_search(fact_chunk, k=3, fetch_k=10)\n",
    "    return [doc.page_content for doc in found_docs]\n",
    "\n",
    "def check_fact_in_web_contexts(question, answer, answer_chunks, web_url):\n",
    "    # Load web document\n",
    "    documents = SimpleWebPageReader(html_to_text=True).load_data([web_url])\n",
    "    index = SummaryIndex.from_documents(documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "    \n",
    "    true_count = 0\n",
    "    fact_statuses = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(answer_chunks, desc=\"Verifying facts\", unit=\"fact\")):\n",
    "        # Get relevant passages from web document\n",
    "        response = query_engine.query(chunk)\n",
    "        \n",
    "        if not response:\n",
    "            fact_statuses.append(\"Unverifiable\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fact {i+1} to be checked: {chunk}\")\n",
    "        print(f\"Relevant Passage: {response}\")\n",
    "\n",
    "        prompt = f\"\"\"Factual Information: {response}\n",
    "        Question: {question}\n",
    "        Is the following statement correct? {chunk}\n",
    "        Answer \"True\" or \"False\" with a brief explanation.\"\"\"\n",
    "        \n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].text.strip()\n",
    "        print(f\"Verification Result: {response_text}\")\n",
    "        \n",
    "        if \"True\" in response_text:\n",
    "            true_count += 1\n",
    "\n",
    "        score = calculate_score(true_count, 1)\n",
    "        fact_statuses.append(\"True\" if score > 0.5 else \"False\")\n",
    "        true_count = 0\n",
    "    \n",
    "    print(\"Final Fact Verification Results:\")\n",
    "    for i, status in enumerate(fact_statuses):\n",
    "        print(f\"Fact {i+1}: {answer_chunks[i]} - {status}\")\n",
    "\n",
    "def main():\n",
    "    question = input(\"Please enter your question: \")\n",
    "    answer = input(\"Please enter your answer: \")\n",
    "    web_url = input(\"Please enter the web URL for context: \")\n",
    "    \n",
    "   \n",
    "    answer_chunks = breakdown_answer(answer)\n",
    "    \n",
    "    print(f\"\\nTotal number of facts identified: {len(answer_chunks)}\\n\")\n",
    "    \n",
    "    check_fact_in_web_contexts(question, answer, answer_chunks, web_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
